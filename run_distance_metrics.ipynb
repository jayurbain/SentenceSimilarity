{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Distance Metrics\n",
    "================\n",
    "\n",
    "Introduces the concept of distance between two bags of words or distributions, and demonstrates its calculation using gensim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply want to calculate the similarity between documents, then you\n",
    "may want to check out the `Similarity Queries Tutorial\n",
    "<https://radimrehurek.com/gensim/tut3.html>`_ and the `API reference\n",
    "<https://radimrehurek.com/gensim/similarities/docsim.html>`_. The current\n",
    "tutorial shows the building block of these larger methods, which are a small\n",
    "suite of distance metrics, including:\n",
    "\n",
    "Here's a brief summary of this tutorial:\n",
    "\n",
    "1. Set up a small corpus consisting of documents belonging to one of two topics\n",
    "2. Train an LDA model to distinguish between the two topics\n",
    "3. Use the model to obtain distributions for some sample words\n",
    "4. Compare the distributions to each other using a variety of distance metrics:\n",
    "\n",
    "  * Hellinger\n",
    "  * Kullback-Leibler\n",
    "  * Jaccard\n",
    "\n",
    "5. Discuss the concept of distance metrics in slightly more detail\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-06 16:08:59,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-06 16:08:59,672 : INFO : built Dictionary(16 unique tokens: ['bank', 'river', 'shore', 'water', 'fast']...) from 11 documents (total 40 corpus positions)\n",
      "2020-03-06 16:08:59,674 : INFO : using symmetric alpha at 0.5\n",
      "2020-03-06 16:08:59,675 : INFO : using symmetric eta at 0.5\n",
      "2020-03-06 16:08:59,676 : INFO : using serial LDA version on this node\n",
      "2020-03-06 16:08:59,679 : INFO : running online (single-pass) LDA training, 2 topics, 1 passes over the supplied corpus of 11 documents, updating model once every 11 documents, evaluating perplexity every 11 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-03-06 16:08:59,679 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2020-03-06 16:08:59,691 : INFO : -3.572 per-word bound, 11.9 perplexity estimate based on a held-out corpus of 11 documents with 40 words\n",
      "2020-03-06 16:08:59,692 : INFO : PROGRESS: pass 0, at document #11/11\n",
      "2020-03-06 16:08:59,703 : INFO : topic #0 (0.500): 0.207*\"bank\" + 0.100*\"water\" + 0.089*\"river\" + 0.088*\"sell\" + 0.067*\"borrow\" + 0.064*\"finance\" + 0.062*\"money\" + 0.053*\"tree\" + 0.045*\"flow\" + 0.044*\"rain\"\n",
      "2020-03-06 16:08:59,704 : INFO : topic #1 (0.500): 0.142*\"bank\" + 0.116*\"water\" + 0.090*\"river\" + 0.084*\"money\" + 0.081*\"finance\" + 0.064*\"flow\" + 0.055*\"transaction\" + 0.055*\"tree\" + 0.053*\"fall\" + 0.050*\"mud\"\n",
      "2020-03-06 16:08:59,705 : INFO : topic diff=0.628554, rho=1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.207*\"bank\" + 0.100*\"water\" + 0.089*\"river\" + 0.088*\"sell\" + 0.067*\"borrow\" + 0.064*\"finance\" + 0.062*\"money\" + 0.053*\"tree\" + 0.045*\"flow\" + 0.044*\"rain\"'),\n",
       " (1,\n",
       "  '0.142*\"bank\" + 0.116*\"water\" + 0.090*\"river\" + 0.084*\"money\" + 0.081*\"finance\" + 0.064*\"flow\" + 0.055*\"transaction\" + 0.055*\"tree\" + 0.053*\"fall\" + 0.050*\"mud\"')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# you can use any corpus, this is just illustratory\n",
    "texts = [\n",
    "    ['bank','river','shore','water'],\n",
    "    ['river','water','flow','fast','tree'],\n",
    "    ['bank','water','fall','flow'],\n",
    "    ['bank','bank','water','rain','river'],\n",
    "    ['river','water','mud','tree'],\n",
    "    ['money','transaction','bank','finance'],\n",
    "    ['bank','borrow','money'], \n",
    "    ['bank','finance'],\n",
    "    ['finance','money','sell','bank'],\n",
    "    ['borrow','sell'],\n",
    "    ['bank','loan','sell'],\n",
    "]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "import numpy\n",
    "numpy.random.seed(1) # setting random seed to get the same results each time.\n",
    "\n",
    "from gensim.models import ldamodel\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2, minimum_probability=1e-8)\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the 1st topic the **water** topic and the second topic the **finance** topic.\n",
    "\n",
    "Let's take a few sample documents and get them ready to test our distance functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_water = ['river', 'water', 'shore']\n",
    "doc_finance = ['finance', 'money', 'sell']\n",
    "doc_bank = ['finance', 'bank', 'tree', 'water']\n",
    "\n",
    "# now let's make these into a bag of words format\n",
    "bow_water = model.id2word.doc2bow(doc_water)   \n",
    "bow_finance = model.id2word.doc2bow(doc_finance)   \n",
    "bow_bank = model.id2word.doc2bow(doc_bank)   \n",
    "\n",
    "# we can now get the LDA topic distributions for these\n",
    "lda_bow_water = model[bow_water]\n",
    "lda_bow_finance = model[bow_finance]\n",
    "lda_bow_bank = model[bow_bank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hellinger\n",
    "---------\n",
    "\n",
    "We're now ready to apply our distance metrics.  These metrics return a value between 0 and 1, where values closer to 0 indicate a smaller 'distance' and therefore a larger similarity.\n",
    "\n",
    "Let's start with the popular Hellinger distance. \n",
    "\n",
    "The Hellinger distance metric gives an output in the range [0,1] for two probability distributions, with values closer to 0 meaning they are more similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24622724526927992\n",
      "0.007332908542502406\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import hellinger\n",
    "print(hellinger(lda_bow_water, lda_bow_finance))\n",
    "print(hellinger(lda_bow_finance, lda_bow_bank))\n",
    "print(hellinger(lda_bow_finance, lda_bow_finance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense, right? In the first example, Document 1 and Document 2 are hardly similar, so we get a value of roughly 0.5. \n",
    "\n",
    "In the second case, the documents are a lot more similar, semantically. Trained with the model, they give a much less distance value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback–Leibler\n",
    "----------------\n",
    "\n",
    "Let's run similar examples down with Kullback Leibler.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import kullback_leibler\n",
    "\n",
    "print(kullback_leibler(lda_bow_water, lda_bow_bank))\n",
    "print(kullback_leibler(lda_bow_finance, lda_bow_bank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. important::\n",
    "  KL is not a Distance Metric in the mathematical sense, and hence is not\n",
    "  symmetrical.  This means that ``kullback_leibler(lda_bow_finance,\n",
    "  lda_bow_bank)`` is not equal to  ``kullback_leibler(lda_bow_bank,\n",
    "  lda_bow_finance)``. \n",
    "\n",
    "As you can see, the values are not equal. We'll get more into the details of\n",
    "this later on in the notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kullback_leibler(lda_bow_bank, lda_bow_finance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous examples we saw that there were lower distance values between\n",
    "bank and finance than for bank and water, even if it wasn't by a huge margin.\n",
    "What does this mean?\n",
    "\n",
    "The ``bank`` document is a combination of both water and finance related\n",
    "terms - but as bank in this context is likely to belong to the finance topic,\n",
    "the distance values are less between the finance and bank bows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to confirm our suspicion that the bank bow is more to do with finance:\n",
    "model.get_document_topics(bow_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident that while it isn't too skewed, it it more towards the finance topic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metrics (also referred to as similarity metrics), as suggested in\n",
    "the examples above, are mainly for probability distributions, but the methods\n",
    "can accept a bunch of formats for input. You can do some further reading on\n",
    "`Kullback Leibler <https://en.wikipedia.org/wiki/Kullback–\n",
    "Leibler_divergence>`_ and `Hellinger\n",
    "<https://en.wikipedia.org/wiki/Hellinger_distance>`_ to figure out what suits\n",
    "your needs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard\n",
    "-------\n",
    "\n",
    "Let us now look at the `Jaccard Distance\n",
    "<https://en.wikipedia.org/wiki/Jaccard_index>`_ metric for similarity between\n",
    "bags of words (i.e, documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import jaccard\n",
    "\n",
    "print(jaccard(bow_water, bow_bank))\n",
    "print(jaccard(doc_water, doc_bank))\n",
    "print(jaccard(['word'], ['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three examples above feature 2 different input methods. \n",
    "\n",
    "In the first case, we present to jaccard document vectors already in bag of\n",
    "words format. The distance can be defined as 1 minus the size of the\n",
    "intersection upon the size of the union of the vectors. \n",
    "\n",
    "We can see (on manual inspection as well), that the distance is likely to be\n",
    "high - and it is. \n",
    "\n",
    "The last two examples illustrate the ability for jaccard to accept even lists\n",
    "(i.e, documents) as inputs.\n",
    "\n",
    "In the last case, because they are the same vectors, the value returned is 0\n",
    "- this means the distance is 0 and the two documents are identical. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Metrics for Topic Distributions\n",
    "----------------------------------------\n",
    "\n",
    "While there are already standard methods to identify similarity of documents,\n",
    "our distance metrics has one more interesting use-case: topic distributions. \n",
    "\n",
    "Let's say we want to find out how similar our two topics are, water and finance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_water, topic_finance = model.show_topics()\n",
    "\n",
    "# some pre processing to get the topics in a format acceptable to our distance metrics\n",
    "\n",
    "def parse_topic_string(topic):\n",
    "    # takes the string returned by model.show_topics()\n",
    "    # split on strings to get topics and the probabilities\n",
    "    topic = topic.split('+')\n",
    "    # list to store topic bows\n",
    "    topic_bow = []\n",
    "    for word in topic:\n",
    "        # split probability and word\n",
    "        prob, word = word.split('*')\n",
    "        # get rid of spaces and quote marks\n",
    "        word = word.replace(\" \",\"\").replace('\"', '')\n",
    "        # convert to word_type\n",
    "        word = model.id2word.doc2bow([word])[0][0]\n",
    "        topic_bow.append((word, float(prob)))\n",
    "    return topic_bow\n",
    "\n",
    "finance_distribution = parse_topic_string(topic_finance[1])\n",
    "water_distribution = parse_topic_string(topic_water[1])\n",
    "\n",
    "# the finance topic in bag of words format looks like this:\n",
    "print(finance_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our topics in a format more acceptable by our functions,\n",
    "let's use a Distance metric to see how similar the word distributions in the\n",
    "topics are.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hellinger(water_distribution, finance_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our value of roughly 0.36 means that the topics are not TOO distant with\n",
    "respect to their word distributions.\n",
    "\n",
    "This makes sense again, because of overlapping words like ``bank`` and a\n",
    "small size dictionary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler Gotchas\n",
    "------------------------\n",
    "\n",
    "In our previous example we didn't use Kullback Leibler to test for similarity\n",
    "for a reason - KL is not a Distance 'Metric' in the technical sense (you can\n",
    "see what a metric is `here\n",
    "<https://en.wikipedia.org/wiki/Metric_(mathematics>`_\\ ). The nature of it,\n",
    "mathematically also means we must be a little careful before using it,\n",
    "because since it involves the log function, a zero can mess things up. For\n",
    "example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 here is the number of features the probability distribution draws from\n",
    "print(kullback_leibler(water_distribution, finance_distribution, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't very helpful, right? This just means that we have to be a bit\n",
    "careful about our inputs. Our old example didn't work out because they were\n",
    "some missing values for some words (because ``show_topics()`` only returned\n",
    "the top 10 topics). \n",
    "\n",
    "This can be remedied, though.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ALL the words in the dictionary for the topic-word distribution.\n",
    "topic_water, topic_finance = model.show_topics(num_words=len(model.id2word))\n",
    "\n",
    "# do our bag of words transformation again\n",
    "finance_distribution = parse_topic_string(topic_finance[1])\n",
    "water_distribution = parse_topic_string(topic_water[1])\n",
    "\n",
    "# and voila!\n",
    "print(kullback_leibler(water_distribution, finance_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the distance for this is quite less, indicating a high\n",
    "similarity. This may be a bit off because of the small size of the corpus,\n",
    "where all topics are likely to contain a decent overlap of word\n",
    "probabilities. You will likely get a better value for a bigger corpus.\n",
    "\n",
    "So, just remember, if you intend to use KL as a metric to measure similarity\n",
    "or distance between two distributions, avoid zeros by returning the ENTIRE\n",
    "distribution. Since it's unlikely any probability distribution will ever have\n",
    "absolute zeros for any feature/word, returning all the values like we did\n",
    "will make you good to go.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Distance Metrics?\n",
    "--------------------------\n",
    "\n",
    "Having seen the practical usages of these measures (i.e, to find similarity),\n",
    "let's learn a little about what exactly Distance Measures and Metrics are. \n",
    "\n",
    "I mentioned in the previous section that KL was not a distance metric. There\n",
    "are 4 conditons for for a distance measure to be a metric:\n",
    "\n",
    "1. d(x,y) >= 0\n",
    "2. d(x,y) = 0 <=> x = y\n",
    "3. d(x,y) = d(y,x)\n",
    "4. d(x,z) <= d(x,y) + d(y,z)\n",
    "\n",
    "That is: it must be non-negative; if x and y are the same, distance must be\n",
    "zero; it must be symmetric; and it must obey the triangle inequality law. \n",
    "\n",
    "Simple enough, right? \n",
    "\n",
    "Let's test these out for our measures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal Hellinger\n",
    "a = hellinger(water_distribution, finance_distribution)\n",
    "b = hellinger(finance_distribution, water_distribution)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a == b)\n",
    "\n",
    "# if we pass the same values, it is zero.\n",
    "print(hellinger(water_distribution, water_distribution))\n",
    "\n",
    "# for triangle inequality let's use LDA document distributions\n",
    "print(hellinger(lda_bow_finance, lda_bow_bank))\n",
    "\n",
    "# Triangle inequality works too!\n",
    "print(hellinger(lda_bow_finance, lda_bow_water) + hellinger(lda_bow_water, lda_bow_bank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Hellinger is indeed a metric. Let's check out KL. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kullback_leibler(finance_distribution, water_distribution)\n",
    "b = kullback_leibler(water_distribution, finance_distribution)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately notice that when we swap the values they aren't equal! One of\n",
    "the four conditions not fitting is enough for it to not be a metric. \n",
    "\n",
    "However, just because it is not a metric, (strictly in the mathematical\n",
    "sense) does not mean that it is not useful to figure out the distance between\n",
    "two probability distributions. KL Divergence is widely used for this purpose,\n",
    "and is probably the most 'famous' distance measure in fields like Information\n",
    "Theory.\n",
    "\n",
    "For a nice review of the mathematical differences between Hellinger and KL,\n",
    "`this\n",
    "<http://stats.stackexchange.com/questions/130432/differences-between-bhattacharyya-distance-and-kl-divergence>`__\n",
    "link does a very good job. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Distance Metrics\n",
    "----------------------------\n",
    "\n",
    "Let's plot a graph of our toy dataset using the popular `networkx\n",
    "<https://networkx.github.io/documentation/stable/>`_ library. \n",
    "\n",
    "Each node will be a document, where the color of the node will be its topic\n",
    "according to the LDA model. Edges will connect documents to each other, where\n",
    "the *weight* of the edge will be inversely proportional to the Jaccard\n",
    "similarity between two documents. We will also annotate the edges to further\n",
    "aid visualization: **strong** edges will connect similar documents, and\n",
    "**weak (dashed)** edges will connect dissimilar documents.\n",
    "\n",
    "In summary, similar documents will be closer together, different documents\n",
    "will be further apart.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "def get_most_likely_topic(doc):\n",
    "    bow = model.id2word.doc2bow(doc)\n",
    "    topics, probabilities = zip(*model.get_document_topics(bow))\n",
    "    max_p = max(probabilities)\n",
    "    topic = topics[probabilities.index(max_p)]\n",
    "    return topic\n",
    "\n",
    "def get_node_color(i):\n",
    "    return 'skyblue' if get_most_likely_topic(texts[i]) == 0 else 'pink'\n",
    "\n",
    "G = nx.Graph()\n",
    "for i, _ in enumerate(texts):\n",
    "    G.add_node(i)\n",
    "    \n",
    "for (i1, i2) in itertools.combinations(range(len(texts)), 2):\n",
    "    bow1, bow2 = texts[i1], texts[i2]\n",
    "    distance = jaccard(bow1, bow2)\n",
    "    G.add_edge(i1, i2, weight=1/distance)\n",
    "    \n",
    "#\n",
    "# https://networkx.github.io/documentation/networkx-1.9/examples/drawing/weighted_graph.html\n",
    "#\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "threshold = 1.25\n",
    "elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] > threshold]\n",
    "esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <= threshold]\n",
    "\n",
    "node_colors = [get_node_color(i) for (i, _) in enumerate(texts)]\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, node_color=node_colors)\n",
    "nx.draw_networkx_edges(G,pos,edgelist=elarge, width=2)\n",
    "nx.draw_networkx_edges(G,pos,edgelist=esmall, width=2, alpha=0.2, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G, pos, font_size=20, font_family='sans-serif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make several observations from this graph.\n",
    "\n",
    "First, the graph consists of two connected components (if you ignore the weak edges).\n",
    "Nodes 0, 1, 2, 3, 4 (which all belong to the water topic) form the first connected component.\n",
    "The other nodes, which all belong to the finance topic, form the second connected component.\n",
    "\n",
    "Second, the LDA model didn't do a very good job of classifying our documents into topics.\n",
    "There were many misclassifications, as you can confirm in the summary below:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('id\\ttopic\\tdoc')\n",
    "for i, t in enumerate(texts):\n",
    "    print('%d\\t%d\\t%s' % (i, get_most_likely_topic(t), ' '.join(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mostly because the corpus used to train the LDA model is so small.\n",
    "Using a larger corpus should give you much better results, but that is beyond\n",
    "the scope of this tutorial.\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "That brings us to the end of this small tutorial.\n",
    "To recap, here's what we covered:\n",
    "\n",
    "1. Set up a small corpus consisting of documents belonging to one of two topics\n",
    "2. Train an LDA model to distinguish between the two topics\n",
    "3. Use the model to obtain distributions for some sample words\n",
    "4. Compare the distributions to each other using a variety of distance metrics: Hellinger, Kullback-Leibler, Jaccard\n",
    "5. Discuss the concept of distance metrics in slightly more detail\n",
    "\n",
    "The scope for adding new similarity metrics is large, as there exist an even\n",
    "larger suite of metrics and methods to add to the matutils.py file.\n",
    "For more details, see `Similarity Measures for Text Document Clustering\n",
    "<http://www.academia.edu/download/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf>`_\n",
    "by A. Huang.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
